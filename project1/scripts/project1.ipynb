{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri0_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_leading_pt\",\"PRI_jet_leading_eta\",\"PRI_jet_leading_phi\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]\n",
    "pri1_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from expansion import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "hb = pd.read_csv(DATA_TRAIN_PATH, sep=',')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "hb = hb.drop(['Id'], 1)\n",
    "#hb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataSet(dataset):\n",
    "    dataset = dataset.replace(-999, np.nan)\n",
    "    pri0 = dataset[dataset.PRI_jet_num==0].copy()\n",
    "    pri0 = pri0.drop(pri0_to_drop,1)\n",
    "    pri0 = pri0.drop([\"PRI_jet_num\",\"PRI_jet_all_pt\"],1)\n",
    "\n",
    "    pri1 = dataset[dataset.PRI_jet_num == 1].copy()\n",
    "    pri1 = pri1.drop(pri1_to_drop,1)\n",
    "    pri1 = pri1.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "    pri2 = dataset[dataset.PRI_jet_num == 2].copy()\n",
    "    pri2 = pri2.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "    pri3 = dataset[dataset.PRI_jet_num == 3].copy()\n",
    "    pri3 = pri3.drop([\"PRI_jet_num\"],1)\n",
    "    \n",
    "    return [pri0,pri1,pri2,pri3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPredictions(dataset):\n",
    "    return dataset.Prediction.apply(lambda x: -1 if x == 'b' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset(dataset):\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset_numpy(dataset):\n",
    "    dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis=0)\n",
    "    dataset = np.nan_to_num(dataset)\n",
    "    dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis = 0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tildaNumpy(X):\n",
    "    return np.c_[np.ones(X.shape[0]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLYNOMIAL_EXPANSION_DEGREE = 9\n",
    "\n",
    "pri = cleanDataSet(hb)\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, dataset in enumerate(pri):\n",
    "    predictions.append(extractPredictions(dataset))\n",
    "    dataset = dataset.drop(['Prediction'],1)\n",
    "    pri[idx] = tildaNumpy(normalizeDataset_numpy(polynomial_expansion( normalizeDataset(dataset).to_numpy(), POLYNOMIAL_EXPANSION_DEGREE)))\n",
    "    # pri[idx] is our tX depending on jet_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DATA cleaning tests*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[1,np.nan,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 3.],\n",
       "       [4., 5., 6.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan_to_num(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  1,  4,  9],\n",
       "       [ 4,  5,  6, 16, 25, 36]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_expansion(test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent.\n",
    "    Uses MSE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n2, e):\n",
    "        return (e.T @ e) / n2\n",
    "    \n",
    "    def compute_gradient(tx, n, e):\n",
    "        return - tx.T @ e / n\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        e = compute_e(y, tx, w)\n",
    "        gradient = compute_gradient(tx, n, e)\n",
    "        loss = compute_loss(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "weights = np.array([])\n",
    "for i in range(100):\n",
    "    initial_w = np.full(tX.shape[1], i/100)\n",
    "    max_iters = 100\n",
    "    gamma = 0.3\n",
    "    w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "    weights = np.append(weights, loss)\n",
    "idx = np.argmin(weights)\n",
    "'''\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.3\n",
    "w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent.\n",
    "    Uses MAE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n, e):\n",
    "        return 1/n * np.sum(np.abs(e))\n",
    "    \n",
    "    def compute_gradient(tx, n, e):\n",
    "        e = y - tx @ w\n",
    "    \n",
    "        return -1/n*tx.T @ np.sign(e)\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        e = compute_e(y, tx, w)\n",
    "        gradient = compute_gradient(tx, n, e)\n",
    "        loss = compute_loss(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.3\n",
    "w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using stochastic gradient descent.\n",
    "    Uses MAE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n2, e):\n",
    "        return (e.T @ e) / n2\n",
    "    \n",
    "    def compute_gradient(tx, n, e):\n",
    "        return - tx.T @ e / n\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w[:, np.newaxis]\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    data_size = len(y)\n",
    "    shuffled_indices = np.random.permutation(np.arange(data_size))\n",
    "    shuffled_y = y[shuffled_indices]\n",
    "    shuffled_tx = tx[shuffled_indices]\n",
    "    shuffled_y = shuffled_y[:,np.newaxis]\n",
    "    for n_iter, by, btx in zip(range(max_iters), shuffled_y, shuffled_tx):\n",
    "        by = by[np.newaxis]\n",
    "        btx = btx[np.newaxis, :]\n",
    "        e = compute_e(by, btx, w)\n",
    "        gradient = compute_gradient(btx, n, e)\n",
    "        loss = compute_loss(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "    return w, compute_loss(n2, compute_e(y, tx, w[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-3c951d01997a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0minitial_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tX' is not defined"
     ]
    }
   ],
   "source": [
    "initial_w = np.full(tX.shape[1], 0.1)\n",
    "max_iters = 100000\n",
    "gamma = 0.7\n",
    "w, loss = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"\n",
    "    Linear regression using normal equations.\n",
    "    Use MSE loss function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"    \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n2, e):\n",
    "        return (e.T @ e) / n2\n",
    "    \n",
    "    w = la.solve(tx.T @ tx, tx.T @ y)\n",
    "    \n",
    "    return w, compute_loss(y.shape[0]*2, compute_e(y, tx, w))\n",
    "\n",
    "w, loss = least_squares(y, tX)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_,_):\n",
    "    \"\"\"\n",
    "    Ridge regression using normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\" \n",
    "    \n",
    "            \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n2, e):\n",
    "        return (e.T @ e) / n2\n",
    "    \n",
    "    \n",
    "    X = tx.T @ tx\n",
    "    w = la.solve(X + lambda_ * (2*y.shape[0]) * np.eye(X.shape[0]), tx.T @ y)\n",
    "    return w, compute_loss(y.shape[0]*2, compute_e(y, tx, w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-735e18d73087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mridge_regression_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tX' is not defined"
     ]
    }
   ],
   "source": [
    "def ridge_regression_demo(tx, y):\n",
    "\n",
    "    \n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    \n",
    "    rmse_tr = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        weights, loss = ridge_regression(y, tx, lambda_)\n",
    "        rmse_tr.append(np.sqrt(loss))\n",
    "\n",
    "        print(\"lambda={l:.3f}, Training RMSE={tr:.3f}\".format(l=lambda_, tr=rmse_tr[ind]))\n",
    "        \n",
    "\n",
    "ridge_regression_demo(tX, y)\n",
    "\n",
    "w, loss = ridge_regression(y, tX, 0.001)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1 + np.exp(-t))\n",
    "    \n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    return np.sum(np.log(1 + np.exp(tx @ w)) - y * (tx @ w))\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T @ (sigmoid(tx@w) - y)\n",
    "\n",
    "\n",
    "def logistic_regression_step(y, tx, w):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    return calculate_loss(y, tx, w), calculate_gradient(y, tx, w)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def logistic_regression(y, tx, lambda_, _):\n",
    "    \"\"\"\n",
    "    Logistic regression using gradient descent or SGD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"  \n",
    "    def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descen using logistic regression.\n",
    "        Return the loss and the updated w.\n",
    "        \"\"\"\n",
    "        loss = calculate_loss(y, tx, w)\n",
    "        gradient = calculate_gradient(y,tx,w)\n",
    "        w = w - gamma * gradient\n",
    "        return loss, w\n",
    "    \n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    #gamma = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    y = y[:,np.newaxis]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "     \n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, losses[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 1e-8\n",
    "w, loss = logistic_regression(y, tX, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def reg_logistic_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression using gradient descent or SGD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"    \n",
    "    def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "        \"\"\"return the loss, gradient\"\"\"\n",
    "        loss, gradient = logistic_regression_step(y, tx, w)\n",
    "        loss     += 2 * lambda_ * la.norm(w)**2\n",
    "        gradient += lambda_ * w\n",
    "\n",
    "        return loss, gradient\n",
    "    \n",
    "    def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descent, using the penalized logistic regression.\n",
    "        Return the loss and updated w.\n",
    "        \"\"\"\n",
    "        loss, gradient = penalized_logistic_regression(y, tx, w, lambda_) \n",
    "        w = w - gamma * gradient \n",
    "\n",
    "        return loss, w\n",
    "    \n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 1e-8\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    y = y[:,np.newaxis]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 0.000001\n",
    "w, loss = reg_logistic_regression(y, tX, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of ML magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9099389914232369\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.full(pri[0].shape[1], 0.1)\n",
    "max_iters = 600\n",
    "gamma = 0.0001\n",
    "w, loss = least_squares_GD(predictions[0].to_numpy(), pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methodes element of this contains [(methode_for_learning,(parameters_of_methode))]\n",
    "'''pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD, (np.zeros(pri[1].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD,(np.zeros(pri[2].shape[1]), 1000, 0.1)), \n",
    "         (least_squares_GD, (np.zeros(pri[3].shape[1]), 1000, 0.1))]\n",
    "'''\n",
    "lamb = 4.64e-06\n",
    "pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "\n",
    "\n",
    "def learn(pri, pri_learn_func):\n",
    "    w_pri = []\n",
    "    losses = []\n",
    "    for idx in range(len(pri)):\n",
    "        learning_function, parameters = pri_learn_func[idx]\n",
    "        w ,loss = learning_function(predictions[idx].to_numpy(),pri[idx],*parameters)\n",
    "        print(\"* \" + str(idx) + \" loss : \" + str(loss))\n",
    "        w_pri.append(w)\n",
    "        losses.append(loss)\n",
    "    return (w_pri, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 loss : 0.23622582833258748\n",
      "* 1 loss : 0.3143502432185906\n",
      "* 2 loss : 0.28563336035676645\n",
      "* 3 loss : 0.2962584122088217\n"
     ]
    }
   ],
   "source": [
    "w_pri, losses = learn(pri,pri_learn_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8692bf415d417bb14d98185658dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_mean = []\n",
    "for i in tqdm(range(0, 1000, 2)):\n",
    "    lamb = i/1000\n",
    "    pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "    w_pri, losses = learn(pri,pri_learn_func)\n",
    "    loss_mean.append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mean.index(np.min(loss_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    " w_pri =w_pri_deg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def p(t,_):\n",
    "    print(_)\n",
    "\n",
    "p(*test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "hbt = pd.read_csv(DATA_TEST_PATH, sep=',')\n",
    "\n",
    "#hbt = hbt.drop(['Id', 'Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbt = hbt.drop(['Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hbt = hbt.set_index(['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pri = cleanDataSet(hbt)\n",
    "test_pri_tX = [] # tX arrays to run prediction on\n",
    "for idx , dataset in enumerate(test_pri):\n",
    "    test_pri_tX.append( tildaNumpy(normalizeDataset_numpy(polynomial_expansion( normalizeDataset(dataset).to_numpy(), POLYNOMIAL_EXPANSION_DEGREE))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_pri,test_pri_tX, w_pri):\n",
    "    for idx, dataset in enumerate(test_pri_tX):\n",
    "        test_pri[idx]['Prediction'] = predict_labels(w_pri[idx],dataset)\n",
    "    return test_pri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = predict(test_pri,test_pri_tX,w_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = pd.concat(test_prediction,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = test_prediction.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv'\n",
    "create_csv_submission(test_prediction.Prediction.keys(), test_prediction.Prediction.values, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "hbt = pd.read_csv(DATA_TEST_PATH, sep=',')\n",
    "hbt = hbt.drop(['Id', 'Prediction'], 1)\n",
    "hbt = hbt.replace(-999, np.nan)\n",
    "hbt = (hbt - hbt.mean()) / hbt.std()\n",
    "hbt = hbt.fillna(0)\n",
    "hbt = (hbt - hbt.mean()) / hbt.std()\n",
    "tX_test = np.c_[np.ones(X_test.shape[0]), hbt.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w[1:], tX_test)#[:, [0, 1, 2, 3, 4, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) # Selected desired columns\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_pred))\n",
    "print(len(y_pred[y_pred > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions scores\n",
    "Best score by technique\n",
    "\n",
    "<ul>\n",
    "    <li>MSE, gradient descent : 0.649</li>\n",
    "    <li>MAE, gradient descent : 0.678 </li>\n",
    "    <li>ridge regression      : 0.664</li>\n",
    "</ul>\n",
    "Best score after not being stupid with bias:\n",
    "\n",
    "* MSE, GD: \n",
    "* MAE, GD: 0.639\n",
    "* LSQ: 0.706\n",
    "* R-REG: 0.730\n",
    "\n",
    "Best score after normalizing test set + putting zero where unknown:\n",
    "\n",
    "* LSQ: 0.747\n",
    "* R-REG: 0.745\n",
    "\n",
    "Feature expansion?\n",
    "\n",
    "Degree polynomial 4, and jet-num separation\n",
    "with these learning parameters:\n",
    "```\n",
    "pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD, (np.zeros(pri[1].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD,(np.zeros(pri[2].shape[1]), 1000, 0.1)), \n",
    "         (least_squares_GD, (np.zeros(pri[3].shape[1]), 1000, 0.1))]\n",
    "```\n",
    "0.801\n",
    "\n",
    "\n",
    "Degree polynomial 9 , jet-num separation\n",
    "learning parameters:\n",
    "```\n",
    "lamb = 4.64e-06\n",
    "pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "```\n",
    "0.779\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
