{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from expansion import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri0_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_leading_pt\",\"PRI_jet_leading_eta\",\"PRI_jet_leading_phi\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]\n",
    "pri1_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "hb = pd.read_csv(DATA_TRAIN_PATH, sep=',')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "hb = hb.drop(['Id'], 1)\n",
    "#hb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataSet(dataset):\n",
    "    dataset = dataset.replace(-999, np.nan)\n",
    "    pri0 = dataset[dataset.PRI_jet_num==0].copy()\n",
    "    pri0 = pri0.drop(pri0_to_drop,1)\n",
    "    pri0 = pri0.drop([\"PRI_jet_num\",\"PRI_jet_all_pt\"],1)\n",
    "\n",
    "    pri1 = dataset[dataset.PRI_jet_num == 1].copy()\n",
    "    pri1 = pri1.drop(pri1_to_drop,1)\n",
    "    pri1 = pri1.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "    pri2 = dataset[dataset.PRI_jet_num == 2].copy()\n",
    "    pri2 = pri2.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "    pri3 = dataset[dataset.PRI_jet_num == 3].copy()\n",
    "    pri3 = pri3.drop([\"PRI_jet_num\"],1)\n",
    "    \n",
    "    return [pri0,pri1,pri2,pri3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPredictions(dataset):\n",
    "    return dataset.Prediction.apply(lambda x: -1 if x == 'b' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset(dataset):\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset_numpy(dataset):\n",
    "    dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis=0)\n",
    "    dataset = np.nan_to_num(dataset)\n",
    "    dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis = 0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tildaNumpy(X):\n",
    "    return np.c_[np.ones(X.shape[0]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLYNOMIAL_EXPANSION_DEGREE = 12\n",
    "\n",
    "pri = cleanDataSet(hb)\n",
    "predictions = []\n",
    "pri_cross_validation_test = []\n",
    "prediction_cross_validation_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''REMOVE SKLEAR FOR CROSS VALIDATION'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "for idx, dataset in enumerate(pri):\n",
    "    predictions.append(extractPredictions(dataset))\n",
    "    dataset = dataset.drop(['Prediction'],1)\n",
    "    pri[idx] = tildaNumpy(normalizeDataset_numpy(polynomial_expansion( normalizeDataset(dataset).to_numpy(), POLYNOMIAL_EXPANSION_DEGREE)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pri[idx],predictions[idx],test_size=0.1, random_state=0)\n",
    "    pri[idx] = X_train\n",
    "    predictions[idx] = y_train\n",
    "    pri_cross_validation_test.append(X_test)\n",
    "    prediction_cross_validation_test.append(y_test)\n",
    "    # pri[idx] is our tX depending on jet_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X, w, y):\n",
    "    return (y == predict_labels(w,X)).sum()/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_e(y, tx, w):\n",
    "    return y - tx @ w\n",
    "\n",
    "def compute_loss_MSE(n2, e):\n",
    "    return (e.T @ e) / n2\n",
    "    \n",
    "def compute_gradient_MSE(tx, n, e):\n",
    "    return - tx.T @ e / n\n",
    "\n",
    "def compute_loss_MAE(n, e):\n",
    "    return 1/n * np.sum(np.abs(e))\n",
    "    \n",
    "def compute_gradient_MAE(tx, n, e):\n",
    "    return -1/n*tx.T @ np.sign(e)\n",
    "\n",
    "def compute_loss_rmse(n2, e):\n",
    "    return np.sqrt(2 * compute_loss_MSE(n2, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent.\n",
    "    Uses MSE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        e = compute_e(y, tx, w)\n",
    "        gradient = compute_gradient_MSE(tx, n, e)\n",
    "        loss = compute_loss_MSE(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bastien/Anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in matmul\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "weights = np.array([])\n",
    "for i in range(100):\n",
    "    initial_w = np.full(tX.shape[1], i/100)\n",
    "    max_iters = 100\n",
    "    gamma = 0.3\n",
    "    w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "    weights = np.append(weights, loss)\n",
    "idx = np.argmin(weights)\n",
    "'''\n",
    "initial_w = np.zeros(pri[0].shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.3\n",
    "w, loss = least_squares_GD(predictions[0], pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent.\n",
    "    Uses MAE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        e = compute_e(y, tx, w)\n",
    "        gradient = compute_gradient_MAE(tx, n, e)\n",
    "        loss = compute_loss_MAE(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4465032089414749\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(pri[0].shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.3\n",
    "w, loss = least_squares_GD(predictions[0], pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using stochastic gradient descent.\n",
    "    Uses MSE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w[:, np.newaxis]\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    data_size = len(y)\n",
    "    shuffled_indices = np.random.permutation(np.arange(data_size))\n",
    "    shuffled_y = y[shuffled_indices]\n",
    "    shuffled_tx = tx[shuffled_indices]\n",
    "    shuffled_y = shuffled_y[:,np.newaxis]\n",
    "    for n_iter, by, btx in zip(range(max_iters), shuffled_y, shuffled_tx):\n",
    "        by = by[np.newaxis]\n",
    "        btx = btx[np.newaxis, :]\n",
    "        e = compute_e(by, btx, w)\n",
    "        gradient = compute_gradient_MSE(btx, n, e)\n",
    "        loss = compute_loss_MSE(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "    return w, compute_loss_MSE(n2, compute_e(y, tx, w[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bastien/Anaconda3/lib/python3.7/site-packages/pandas/core/series.py:942: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.full(pri[0].shape[1], 0.1)\n",
    "max_iters = 100000\n",
    "gamma = 0.7\n",
    "w, loss = least_squares_SGD(predictions[0], pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"\n",
    "    Linear regression using normal equations.\n",
    "    Use MSE loss function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"    \n",
    "    \n",
    "    w = la.solve(tx.T @ tx, tx.T @ y)\n",
    "    \n",
    "    return w, compute_loss_MSE(y.shape[0]*2, compute_e(y, tx, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.15658150782815\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(predictions[0], pri[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression\n",
    "with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Ridge regression using normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\" \n",
    "     \n",
    "    X = tx.T @ tx\n",
    "    n = y.shape[0]\n",
    "    w = la.solve(X + lambda_ * (2 * n) * np.eye(X.shape[0]), tx.T @ y)\n",
    "    return w, compute_loss_rmse(2 * n, compute_e(y, tx, w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # split the data based on the given ratio\n",
    "    # ***************************************************\n",
    "    N = x.shape[0]\n",
    "    indices_training = np.random.choice(N, (int)(ratio*N),replace=False)\n",
    "    mask_training = np.zeros(N, dtype=bool)\n",
    "    mask_training[indices_training] = True\n",
    "    mask_testing = ~mask_training\n",
    "    return (x[mask_training], x[mask_testing], y[mask_training], y[mask_testing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import *\n",
    "def ridge_regression_cross(x, y, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-3, 0, 30)\n",
    "    # ***************************************************\n",
    "    # split the data, and return train and test data\n",
    "    # ***************************************************\n",
    "    x_train, x_test, y_train, y_test = split_data(x,y, ratio, seed)\n",
    "    \n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ***************************************************\n",
    "        # ridge regression with a given lambda\n",
    "        # ***************************************************\n",
    "        weights, rmse = ridge_regression(y_train, x_train, lambda_)\n",
    "        rmse_tr.append(rmse)\n",
    "        rmse_te.append(compute_loss_rmse(y_test.shape[0]*2, compute_e(y_test,x_test, weights)))\n",
    "\n",
    "        print(\"proportion={p}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               p=ratio, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.8, lambda=0.001, Training RMSE=0.695, Testing RMSE=1.151\n",
      "proportion=0.8, lambda=0.001, Training RMSE=0.695, Testing RMSE=0.977\n",
      "proportion=0.8, lambda=0.002, Training RMSE=0.696, Testing RMSE=0.834\n",
      "proportion=0.8, lambda=0.002, Training RMSE=0.696, Testing RMSE=0.737\n",
      "proportion=0.8, lambda=0.003, Training RMSE=0.697, Testing RMSE=0.696\n",
      "proportion=0.8, lambda=0.003, Training RMSE=0.697, Testing RMSE=0.703\n",
      "proportion=0.8, lambda=0.004, Training RMSE=0.698, Testing RMSE=0.741\n",
      "proportion=0.8, lambda=0.005, Training RMSE=0.699, Testing RMSE=0.792\n",
      "proportion=0.8, lambda=0.007, Training RMSE=0.699, Testing RMSE=0.842\n",
      "proportion=0.8, lambda=0.009, Training RMSE=0.700, Testing RMSE=0.882\n",
      "proportion=0.8, lambda=0.011, Training RMSE=0.701, Testing RMSE=0.910\n",
      "proportion=0.8, lambda=0.014, Training RMSE=0.702, Testing RMSE=0.923\n",
      "proportion=0.8, lambda=0.017, Training RMSE=0.704, Testing RMSE=0.922\n",
      "proportion=0.8, lambda=0.022, Training RMSE=0.705, Testing RMSE=0.909\n",
      "proportion=0.8, lambda=0.028, Training RMSE=0.707, Testing RMSE=0.887\n",
      "proportion=0.8, lambda=0.036, Training RMSE=0.709, Testing RMSE=0.859\n",
      "proportion=0.8, lambda=0.045, Training RMSE=0.712, Testing RMSE=0.830\n",
      "proportion=0.8, lambda=0.057, Training RMSE=0.715, Testing RMSE=0.803\n",
      "proportion=0.8, lambda=0.073, Training RMSE=0.718, Testing RMSE=0.780\n",
      "proportion=0.8, lambda=0.092, Training RMSE=0.722, Testing RMSE=0.763\n",
      "proportion=0.8, lambda=0.117, Training RMSE=0.727, Testing RMSE=0.753\n",
      "proportion=0.8, lambda=0.149, Training RMSE=0.733, Testing RMSE=0.749\n",
      "proportion=0.8, lambda=0.189, Training RMSE=0.741, Testing RMSE=0.750\n",
      "proportion=0.8, lambda=0.240, Training RMSE=0.750, Testing RMSE=0.755\n",
      "proportion=0.8, lambda=0.304, Training RMSE=0.760, Testing RMSE=0.764\n",
      "proportion=0.8, lambda=0.386, Training RMSE=0.772, Testing RMSE=0.775\n",
      "proportion=0.8, lambda=0.489, Training RMSE=0.787, Testing RMSE=0.789\n",
      "proportion=0.8, lambda=0.621, Training RMSE=0.802, Testing RMSE=0.804\n",
      "proportion=0.8, lambda=0.788, Training RMSE=0.819, Testing RMSE=0.820\n",
      "proportion=0.8, lambda=1.000, Training RMSE=0.836, Testing RMSE=0.838\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge_regression_cross(pri[0], predictions[0], 0.8, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    x_tr = np.delete(x, k_indices[k], axis=0)\n",
    "    y_tr = np.delete(y, k_indices[k], axis=0)\n",
    "    \n",
    "    x_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    w, rmse = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    \n",
    "    n = y.shape[0]\n",
    "    loss_tr = np.sqrt(rmse)\n",
    "    loss_te = np.sqrt(2 * compute_loss_MSE(2*n , compute_e(y_te, x_te, w)))\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-9, -7, 30)\n",
    "    \n",
    "    y = predictions[0]\n",
    "    y = y[:,np.newaxis]\n",
    "    x = pri[0]\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    var_tr = []\n",
    "    var_te = []\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr = []\n",
    "        loss_te = []\n",
    "        for k in range(k_fold):\n",
    "            tr, te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            loss_tr.append(tr)\n",
    "            loss_te.append(te)\n",
    "        rmse_tr.append(np.mean(loss_tr))\n",
    "        rmse_te.append(np.mean(loss_te))\n",
    "        var_tr.append(np.std(loss_tr))\n",
    "        var_te.append(np.std(loss_te))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te, var_tr, var_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1 + np.exp(-t))\n",
    "    \n",
    "def compute_loss_logistic(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    return np.sum(np.log(1 + np.exp(tx @ w)) - y * (tx @ w))\n",
    "\n",
    "\n",
    "def compute_gradient_logistic(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T @ (sigmoid(tx @ w) - y)\n",
    "\n",
    "\n",
    "def logistic_regression_step(y, tx, w):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    return compute_loss_logistic(y, tx, w), compute_gradient_logistic(y, tx, w)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def logistic_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Logistic regression using gradient descent or SGD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"  \n",
    "    def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descen using logistic regression.\n",
    "        Return the loss and the updated w.\n",
    "        \"\"\"\n",
    "        loss, gradient = logistic_regression_step(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        return loss, w\n",
    "    \n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    #gamma = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    y = y[:,np.newaxis]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "     \n",
    "    print(\"loss={l}\".format(l=compute_loss_logistic(y, tx, w)))\n",
    "    return w, losses[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=62328.487623130844\n",
      "Current iteration=100, loss=48973.81914445322\n",
      "Current iteration=200, loss=37338.256705392734\n",
      "Current iteration=300, loss=26872.57981274616\n",
      "Current iteration=400, loss=17245.077432882572\n",
      "Current iteration=500, loss=8249.602539250398\n",
      "Current iteration=600, loss=-248.31740935856556\n",
      "Current iteration=700, loss=-8340.885215389557\n",
      "Current iteration=800, loss=-16094.27607548367\n",
      "Current iteration=900, loss=-23557.94346503339\n",
      "Current iteration=1000, loss=-30770.171562482265\n",
      "Current iteration=1100, loss=-37761.51356869888\n",
      "Current iteration=1200, loss=-44556.990240726744\n",
      "Current iteration=1300, loss=-51177.53529141257\n",
      "Current iteration=1400, loss=-57640.970011219055\n",
      "Current iteration=1500, loss=-63962.67691413828\n",
      "Current iteration=1600, loss=-70156.07743936771\n",
      "Current iteration=1700, loss=-76232.98001459955\n",
      "Current iteration=1800, loss=-82203.84092925806\n",
      "Current iteration=1900, loss=-88077.9654617024\n",
      "Current iteration=2000, loss=-93863.66715096396\n",
      "Current iteration=2100, loss=-99568.3969836271\n",
      "Current iteration=2200, loss=-105198.85033661386\n",
      "Current iteration=2300, loss=-110761.0569895436\n",
      "Current iteration=2400, loss=-116260.45788984692\n",
      "Current iteration=2500, loss=-121701.97129329154\n",
      "Current iteration=2600, loss=-127090.0502025646\n",
      "Current iteration=2700, loss=-132428.7325544955\n",
      "Current iteration=2800, loss=-137721.6852797836\n",
      "Current iteration=2900, loss=-142972.243126745\n",
      "Current iteration=3000, loss=-148183.44297102888\n",
      "Current iteration=3100, loss=-153358.05420655932\n",
      "Current iteration=3200, loss=-158498.6057161561\n",
      "Current iteration=3300, loss=-163607.4098446902\n",
      "Current iteration=3400, loss=-168686.58373729087\n",
      "Current iteration=3500, loss=-173738.06835592896\n",
      "Current iteration=3600, loss=-178763.64544683302\n",
      "Current iteration=3700, loss=-183764.95269668073\n",
      "Current iteration=3800, loss=-188743.49728599912\n",
      "Current iteration=3900, loss=-193700.6680227383\n",
      "Current iteration=4000, loss=-198637.7462168621\n",
      "Current iteration=4100, loss=-203555.91543751163\n",
      "Current iteration=4200, loss=-208456.27027742166\n",
      "Current iteration=4300, loss=-213339.82423450687\n",
      "Current iteration=4400, loss=-218207.51680757714\n",
      "Current iteration=4500, loss=-223060.2198917913\n",
      "Current iteration=4600, loss=-227898.74354948642\n",
      "Current iteration=4700, loss=-232723.84122327607\n",
      "Current iteration=4800, loss=-237536.21445062023\n",
      "Current iteration=4900, loss=-242336.51713232324\n",
      "Current iteration=5000, loss=-247125.35940147116\n",
      "Current iteration=5100, loss=-251903.31113409714\n",
      "Current iteration=5200, loss=-256670.90513825775\n",
      "Current iteration=5300, loss=-261428.6400541465\n",
      "Current iteration=5400, loss=-266176.98299428716\n",
      "Current iteration=5500, loss=-270916.3719496877\n",
      "Current iteration=5600, loss=-275647.21798503905\n",
      "Current iteration=5700, loss=-280369.9072435675\n",
      "Current iteration=5800, loss=-285084.8027799578\n",
      "Current iteration=5900, loss=-289792.2462378193\n",
      "Current iteration=6000, loss=-294492.559386446\n",
      "Current iteration=6100, loss=-299186.0455300847\n",
      "Current iteration=6200, loss=-303872.990801575\n",
      "Current iteration=6300, loss=-308553.6653509961\n",
      "Current iteration=6400, loss=-313228.3244389009\n",
      "Current iteration=6500, loss=-317897.20944273815\n",
      "Current iteration=6600, loss=-322560.54878421413\n",
      "Current iteration=6700, loss=-327218.55878458905\n",
      "Current iteration=6800, loss=-331871.4444542069\n",
      "Current iteration=6900, loss=-336519.4002219589\n",
      "Current iteration=7000, loss=-341162.6106098291\n",
      "Current iteration=7100, loss=-345801.2508571839\n",
      "Current iteration=7200, loss=-350435.48749903054\n",
      "Current iteration=7300, loss=-355065.47890207474\n",
      "Current iteration=7400, loss=-359691.37576205184\n",
      "Current iteration=7500, loss=-364313.32156549935\n",
      "Current iteration=7600, loss=-368931.45301883755\n",
      "Current iteration=7700, loss=-373545.90044738416\n",
      "Current iteration=7800, loss=-378156.78816668264\n",
      "Current iteration=7900, loss=-382764.23482832505\n",
      "Current iteration=8000, loss=-387368.3537422573\n",
      "Current iteration=8100, loss=-391969.25317738106\n",
      "Current iteration=8200, loss=-396567.0366421149\n",
      "Current iteration=8300, loss=-401161.8031464374\n",
      "Current iteration=8400, loss=-405753.64744680264\n",
      "Current iteration=8500, loss=-410342.66027521\n",
      "Current iteration=8600, loss=-414928.92855359515\n",
      "Current iteration=8700, loss=-419512.5355946219\n",
      "Current iteration=8800, loss=-424093.5612898624\n",
      "Current iteration=8900, loss=-428672.0822862749\n",
      "Current iteration=9000, loss=-433248.1721518159\n",
      "Current iteration=9100, loss=-437821.90153095155\n",
      "Current iteration=9200, loss=-442393.33829078404\n",
      "Current iteration=9300, loss=-446962.54765844083\n",
      "Current iteration=9400, loss=-451529.59235033207\n",
      "Current iteration=9500, loss=-456094.532693828\n",
      "Current iteration=9600, loss=-460657.42674187524\n",
      "Current iteration=9700, loss=-465218.33038102067\n",
      "Current iteration=9800, loss=-469777.2974332859\n",
      "Current iteration=9900, loss=-474334.37975229183\n",
      "loss=-478889.6273140165\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 1e-8\n",
    "w, loss = logistic_regression(predictions[0], pri[0], lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def reg_logistic_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression using gradient descent or SGD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"    \n",
    "    def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "        \"\"\"return the loss, gradient\"\"\"\n",
    "        loss, gradient = logistic_regression_step(y, tx, w)\n",
    "        loss     += 2 * lambda_ * la.norm(w)**2\n",
    "        gradient += lambda_ * w\n",
    "\n",
    "        return loss, gradient\n",
    "    \n",
    "    def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descent, using the penalized logistic regression.\n",
    "        Return the loss and updated w.\n",
    "        \"\"\"\n",
    "        loss, gradient = penalized_logistic_regression(y, tx, w, lambda_) \n",
    "        w = w - gamma * gradient \n",
    "\n",
    "        return loss, w\n",
    "    \n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 1e-8\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    y = y[:,np.newaxis]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    print(\"loss={l}\".format(l=compute_loss_logistic(y, tx, w)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=62328.487623130844\n",
      "Current iteration=100, loss=48973.8191444862\n",
      "Current iteration=200, loss=37338.25670551508\n",
      "Current iteration=300, loss=26872.579813004617\n",
      "Current iteration=400, loss=17245.077433318063\n",
      "Current iteration=500, loss=8249.602539900077\n",
      "Current iteration=600, loss=-248.31740846014515\n",
      "Current iteration=700, loss=-8340.885214209806\n",
      "Current iteration=800, loss=-16094.276073991557\n",
      "Current iteration=900, loss=-23557.94346319905\n",
      "Current iteration=1000, loss=-30770.171560276955\n",
      "Current iteration=1100, loss=-37761.51356609462\n",
      "Current iteration=1200, loss=-44556.99023769629\n",
      "Current iteration=1300, loss=-51177.53528792933\n",
      "Current iteration=1400, loss=-57640.970007257034\n",
      "Current iteration=1500, loss=-63962.67690967193\n",
      "Current iteration=1600, loss=-70156.07743437188\n",
      "Current iteration=1700, loss=-76232.98000904938\n",
      "Current iteration=1800, loss=-82203.8409231291\n",
      "Current iteration=1900, loss=-88077.9654549703\n",
      "Current iteration=2000, loss=-93863.66714360463\n",
      "Current iteration=2100, loss=-99568.39697561684\n",
      "Current iteration=2200, loss=-105198.85032792894\n",
      "Current iteration=2300, loss=-110761.05698016044\n",
      "Current iteration=2400, loss=-116260.45787974211\n",
      "Current iteration=2500, loss=-121701.97128244165\n",
      "Current iteration=2600, loss=-127090.05019094644\n",
      "Current iteration=2700, loss=-132428.73254208607\n",
      "Current iteration=2800, loss=-137721.6852665598\n",
      "Current iteration=2900, loss=-142972.2431126837\n",
      "Current iteration=3000, loss=-148183.4429561072\n",
      "Current iteration=3100, loss=-153358.0541907545\n",
      "Current iteration=3200, loss=-158498.6056994455\n",
      "Current iteration=3300, loss=-163607.40982705101\n",
      "Current iteration=3400, loss=-168686.58371870007\n",
      "Current iteration=3500, loss=-173738.06833636377\n",
      "Current iteration=3600, loss=-178763.6454262709\n",
      "Current iteration=3700, loss=-183764.95267509882\n",
      "Current iteration=3800, loss=-188743.49726337494\n",
      "Current iteration=3900, loss=-193700.66799904912\n",
      "Current iteration=4000, loss=-198637.74619208553\n",
      "Current iteration=4100, loss=-203555.91541162474\n",
      "Current iteration=4200, loss=-208456.27025040198\n",
      "Current iteration=4300, loss=-213339.8242063317\n",
      "Current iteration=4400, loss=-218207.51677822392\n",
      "Current iteration=4500, loss=-223060.21986123757\n",
      "Current iteration=4600, loss=-227898.74351770975\n",
      "Current iteration=4700, loss=-232723.84119025382\n",
      "Current iteration=4800, loss=-237536.21441632995\n",
      "Current iteration=4900, loss=-242336.5170967423\n",
      "Current iteration=5000, loss=-247125.35936457705\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-c9750de92542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.000001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-dd92759d7cd9>\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_penalized_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-dd92759d7cd9>\u001b[0m in \u001b[0;36mlearning_by_penalized_gradient\u001b[0;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-dd92759d7cd9>\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m\"\"\"return the loss, gradient\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m     \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-d814ff731ab4>\u001b[0m in \u001b[0;36mlogistic_regression_step\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_loss_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_gradient_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-d814ff731ab4>\u001b[0m in \u001b[0;36mcompute_gradient_logistic\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradient_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_ = 0.000001\n",
    "w, loss = reg_logistic_regression(predictions[0], pri[0], lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of ML magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9099389914232369\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.full(pri[0].shape[1], 0.1)\n",
    "max_iters = 600\n",
    "gamma = 0.0001\n",
    "w, loss = least_squares_GD(predictions[0].to_numpy(), pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methodes element of this contains [(methode_for_learning,(parameters_of_methode))]\n",
    "'''pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD, (np.zeros(pri[1].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD,(np.zeros(pri[2].shape[1]), 1000, 0.1)), \n",
    "         (least_squares_GD, (np.zeros(pri[3].shape[1]), 1000, 0.1))]\n",
    "'''\n",
    "'''\n",
    "lamb = 4.64e-06\n",
    "pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "'''\n",
    "pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD, (np.zeros(pri[1].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD,(np.zeros(pri[2].shape[1]), 1000, 0.1)), \n",
    "         (least_squares_GD, (np.zeros(pri[3].shape[1]), 1000, 0.1))]\n",
    "\n",
    "\n",
    "def learn(pri, pri_learn_func):\n",
    "    w_pri = []\n",
    "    losses = []\n",
    "    for idx in range(len(pri)):\n",
    "        learning_function, parameters = pri_learn_func[idx]\n",
    "        w ,loss = learning_function(predictions[idx].to_numpy(),pri[idx],*parameters)\n",
    "        print(\"* \" + str(idx) + \" loss : \" + str(loss))\n",
    "        w_pri.append(w)\n",
    "        losses.append(loss)\n",
    "    return (w_pri, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 loss : 0.2552065430748501\n",
      "* 1 loss : 0.33465330174785973\n",
      "* 2 loss : 0.3322111956675665\n",
      "* 3 loss : 0.33545285144330905\n"
     ]
    }
   ],
   "source": [
    "w_pri, losses = learn(pri,pri_learn_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_pri[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_attempt(pri_cross_validation_test, w_pri, prediction_cross_validation_test):\n",
    "    res = []\n",
    "    for i in range(len(w_pri)):\n",
    "        s = score(pri_cross_validation_test[i], w_pri[i], prediction_cross_validation_test[i])\n",
    "        res.append(s)\n",
    "        print(\"Socre for pri : \" +  str(i) + \" is : \"+ str(s))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socre for pri : 0 is : 0.8034427542033626\n",
      "Socre for pri : 1 is : 0.7361702127659574\n",
      "Socre for pri : 2 is : 0.7854307264787614\n",
      "Socre for pri : 3 is : 0.7275597654488047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8034427542033626,\n",
       " 0.7361702127659574,\n",
       " 0.7854307264787614,\n",
       " 0.7275597654488047]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_attempt(pri_cross_validation_test, w_pri, prediction_cross_validation_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7361702127659574"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(pri_cross_validation_test[1], w_pri[1], prediction_cross_validation_test[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8692bf415d417bb14d98185658dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_mean = []\n",
    "for i in tqdm(range(0, 1000, 2)):\n",
    "    lamb = i/1000\n",
    "    pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "    w_pri, losses = learn(pri,pri_learn_func)\n",
    "    loss_mean.append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mean.index(np.min(loss_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    " w_pri =w_pri_deg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def p(t,_):\n",
    "    print(_)\n",
    "\n",
    "p(*test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "hbt = pd.read_csv(DATA_TEST_PATH, sep=',')\n",
    "\n",
    "hbt = hbt.drop(['Prediction'], 1)\n",
    "\n",
    "hbt = hbt.set_index(['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pri = cleanDataSet(hbt)\n",
    "test_pri_tX = [] # tX arrays to run prediction on\n",
    "for idx , dataset in enumerate(test_pri):\n",
    "    test_pri_tX.append( tildaNumpy(normalizeDataset_numpy(polynomial_expansion( normalizeDataset(dataset).to_numpy(), POLYNOMIAL_EXPANSION_DEGREE))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_pri,test_pri_tX, w_pri):\n",
    "    for idx, dataset in enumerate(test_pri_tX):\n",
    "        test_pri[idx]['Prediction'] = predict_labels(w_pri[idx],dataset)\n",
    "    return test_pri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = predict(test_pri,test_pri_tX,w_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = pd.concat(test_prediction,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = test_prediction.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv'\n",
    "create_csv_submission(test_prediction.Prediction.keys(), test_prediction.Prediction.values, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "hbt = pd.read_csv(DATA_TEST_PATH, sep=',')\n",
    "hbt = hbt.drop(['Id', 'Prediction'], 1)\n",
    "hbt = hbt.replace(-999, np.nan)\n",
    "hbt = (hbt - hbt.mean()) / hbt.std()\n",
    "hbt = hbt.fillna(0)\n",
    "hbt = (hbt - hbt.mean()) / hbt.std()\n",
    "tX_test = np.c_[np.ones(X_test.shape[0]), hbt.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w[1:], tX_test)#[:, [0, 1, 2, 3, 4, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) # Selected desired columns\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_pred))\n",
    "print(len(y_pred[y_pred > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions scores\n",
    "Best score by technique\n",
    "\n",
    "<ul>\n",
    "    <li>MSE, gradient descent : 0.649</li>\n",
    "    <li>MAE, gradient descent : 0.678 </li>\n",
    "    <li>ridge regression      : 0.664</li>\n",
    "</ul>\n",
    "Best score after not being stupid with bias:\n",
    "\n",
    "* MSE, GD: \n",
    "* MAE, GD: 0.639\n",
    "* LSQ: 0.706\n",
    "* R-REG: 0.730\n",
    "\n",
    "Best score after normalizing test set + putting zero where unknown:\n",
    "\n",
    "* LSQ: 0.747\n",
    "* R-REG: 0.745\n",
    "\n",
    "Feature expansion?\n",
    "\n",
    "Degree polynomial 4, and jet-num separation MAE\n",
    "with these learning parameters:\n",
    "```\n",
    "pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD, (np.zeros(pri[1].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD,(np.zeros(pri[2].shape[1]), 1000, 0.1)), \n",
    "         (least_squares_GD, (np.zeros(pri[3].shape[1]), 1000, 0.1))]\n",
    "```\n",
    "0.801\n",
    "\n",
    "\n",
    "Degree polynomial 9 , jet-num separation\n",
    "learning parameters:\n",
    "```\n",
    "lamb = 4.64e-06\n",
    "pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "```\n",
    "0.779\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
