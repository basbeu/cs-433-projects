{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n",
      "85667\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tX = np.c_[np.ones(X.shape[0]),X]\n",
    "print(len(y))\n",
    "print(len(y[y > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri0_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_leading_pt\",\"PRI_jet_leading_eta\",\"PRI_jet_leading_phi\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]\n",
    "pri1_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>DER_sum_pt</th>\n",
       "      <th>DER_pt_ratio_lep_tau</th>\n",
       "      <th>DER_met_phi_centrality</th>\n",
       "      <th>DER_lep_eta_centrality</th>\n",
       "      <th>PRI_tau_pt</th>\n",
       "      <th>PRI_tau_eta</th>\n",
       "      <th>PRI_tau_phi</th>\n",
       "      <th>PRI_lep_pt</th>\n",
       "      <th>PRI_lep_eta</th>\n",
       "      <th>PRI_lep_phi</th>\n",
       "      <th>PRI_met</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-49.023079</td>\n",
       "      <td>49.239819</td>\n",
       "      <td>81.181982</td>\n",
       "      <td>57.895962</td>\n",
       "      <td>-708.420675</td>\n",
       "      <td>-601.237051</td>\n",
       "      <td>-709.356603</td>\n",
       "      <td>2.373100</td>\n",
       "      <td>18.917332</td>\n",
       "      <td>158.432217</td>\n",
       "      <td>1.437609</td>\n",
       "      <td>-0.128305</td>\n",
       "      <td>-708.985189</td>\n",
       "      <td>38.707419</td>\n",
       "      <td>-0.010973</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>46.660207</td>\n",
       "      <td>-0.019507</td>\n",
       "      <td>0.043543</td>\n",
       "      <td>41.717235</td>\n",
       "      <td>-0.010119</td>\n",
       "      <td>209.797178</td>\n",
       "      <td>0.979176</td>\n",
       "      <td>-348.329567</td>\n",
       "      <td>-399.254314</td>\n",
       "      <td>-399.259788</td>\n",
       "      <td>-692.381204</td>\n",
       "      <td>-709.121609</td>\n",
       "      <td>-709.118631</td>\n",
       "      <td>73.064591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>406.345647</td>\n",
       "      <td>35.344886</td>\n",
       "      <td>40.828691</td>\n",
       "      <td>63.655682</td>\n",
       "      <td>454.480565</td>\n",
       "      <td>657.972302</td>\n",
       "      <td>453.019877</td>\n",
       "      <td>0.782911</td>\n",
       "      <td>22.273494</td>\n",
       "      <td>115.706115</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>1.193585</td>\n",
       "      <td>453.596721</td>\n",
       "      <td>22.412081</td>\n",
       "      <td>1.214079</td>\n",
       "      <td>1.816763</td>\n",
       "      <td>22.064922</td>\n",
       "      <td>1.264982</td>\n",
       "      <td>1.816611</td>\n",
       "      <td>32.894693</td>\n",
       "      <td>1.812223</td>\n",
       "      <td>126.499506</td>\n",
       "      <td>0.977426</td>\n",
       "      <td>532.962789</td>\n",
       "      <td>489.338286</td>\n",
       "      <td>489.333883</td>\n",
       "      <td>479.875496</td>\n",
       "      <td>453.384624</td>\n",
       "      <td>453.389017</td>\n",
       "      <td>98.015662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.329000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.104000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>-1.414000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>-2.499000</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>-2.505000</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>13.678000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>78.100750</td>\n",
       "      <td>19.241000</td>\n",
       "      <td>59.388750</td>\n",
       "      <td>14.068750</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>2.841000</td>\n",
       "      <td>77.550000</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>-1.371000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>24.591750</td>\n",
       "      <td>-0.925000</td>\n",
       "      <td>-1.575000</td>\n",
       "      <td>32.375000</td>\n",
       "      <td>-1.014000</td>\n",
       "      <td>-1.522000</td>\n",
       "      <td>21.398000</td>\n",
       "      <td>-1.575000</td>\n",
       "      <td>123.017500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>105.012000</td>\n",
       "      <td>46.524000</td>\n",
       "      <td>73.752000</td>\n",
       "      <td>38.467500</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>2.491500</td>\n",
       "      <td>12.315500</td>\n",
       "      <td>120.664500</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>-0.356000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>31.804000</td>\n",
       "      <td>-0.023000</td>\n",
       "      <td>-0.033000</td>\n",
       "      <td>40.516000</td>\n",
       "      <td>-0.045000</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>34.802000</td>\n",
       "      <td>-0.024000</td>\n",
       "      <td>179.739000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.960000</td>\n",
       "      <td>-1.872000</td>\n",
       "      <td>-2.093000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>40.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>130.606250</td>\n",
       "      <td>73.598000</td>\n",
       "      <td>92.259000</td>\n",
       "      <td>79.169000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>83.446000</td>\n",
       "      <td>-4.593000</td>\n",
       "      <td>2.961000</td>\n",
       "      <td>27.591000</td>\n",
       "      <td>200.478250</td>\n",
       "      <td>1.777000</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.017000</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>1.565000</td>\n",
       "      <td>53.390000</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>1.618000</td>\n",
       "      <td>51.895000</td>\n",
       "      <td>1.561000</td>\n",
       "      <td>263.379250</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>75.349000</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>33.703000</td>\n",
       "      <td>-2.457000</td>\n",
       "      <td>-2.275000</td>\n",
       "      <td>109.933750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1192.026000</td>\n",
       "      <td>690.075000</td>\n",
       "      <td>1349.351000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>8.503000</td>\n",
       "      <td>4974.979000</td>\n",
       "      <td>16.690000</td>\n",
       "      <td>5.684000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>1852.462000</td>\n",
       "      <td>19.773000</td>\n",
       "      <td>1.414000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>764.408000</td>\n",
       "      <td>2.497000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>560.271000</td>\n",
       "      <td>2.503000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>2842.617000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>2003.976000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1120.573000</td>\n",
       "      <td>4.499000</td>\n",
       "      <td>3.141000</td>\n",
       "      <td>721.456000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>1633.433000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DER_mass_MMC  DER_mass_transverse_met_lep   DER_mass_vis  \\\n",
       "count  250000.000000                250000.000000  250000.000000   \n",
       "mean      -49.023079                    49.239819      81.181982   \n",
       "std       406.345647                    35.344886      40.828691   \n",
       "min      -999.000000                     0.000000       6.329000   \n",
       "25%        78.100750                    19.241000      59.388750   \n",
       "50%       105.012000                    46.524000      73.752000   \n",
       "75%       130.606250                    73.598000      92.259000   \n",
       "max      1192.026000                   690.075000    1349.351000   \n",
       "\n",
       "            DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  \\\n",
       "count  250000.000000         250000.000000     250000.000000   \n",
       "mean       57.895962           -708.420675       -601.237051   \n",
       "std        63.655682            454.480565        657.972302   \n",
       "min         0.000000           -999.000000       -999.000000   \n",
       "25%        14.068750           -999.000000       -999.000000   \n",
       "50%        38.467500           -999.000000       -999.000000   \n",
       "75%        79.169000              0.490000         83.446000   \n",
       "max      2834.999000              8.503000       4974.979000   \n",
       "\n",
       "       DER_prodeta_jet_jet  DER_deltar_tau_lep     DER_pt_tot     DER_sum_pt  \\\n",
       "count        250000.000000       250000.000000  250000.000000  250000.000000   \n",
       "mean           -709.356603            2.373100      18.917332     158.432217   \n",
       "std             453.019877            0.782911      22.273494     115.706115   \n",
       "min            -999.000000            0.208000       0.000000      46.104000   \n",
       "25%            -999.000000            1.810000       2.841000      77.550000   \n",
       "50%            -999.000000            2.491500      12.315500     120.664500   \n",
       "75%              -4.593000            2.961000      27.591000     200.478250   \n",
       "max              16.690000            5.684000    2834.999000    1852.462000   \n",
       "\n",
       "       DER_pt_ratio_lep_tau  DER_met_phi_centrality  DER_lep_eta_centrality  \\\n",
       "count         250000.000000           250000.000000           250000.000000   \n",
       "mean               1.437609               -0.128305             -708.985189   \n",
       "std                0.844743                1.193585              453.596721   \n",
       "min                0.047000               -1.414000             -999.000000   \n",
       "25%                0.883000               -1.371000             -999.000000   \n",
       "50%                1.280000               -0.356000             -999.000000   \n",
       "75%                1.777000                1.225000                0.000000   \n",
       "max               19.773000                1.414000                1.000000   \n",
       "\n",
       "          PRI_tau_pt    PRI_tau_eta    PRI_tau_phi     PRI_lep_pt  \\\n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000   \n",
       "mean       38.707419      -0.010973      -0.008171      46.660207   \n",
       "std        22.412081       1.214079       1.816763      22.064922   \n",
       "min        20.000000      -2.499000      -3.142000      26.000000   \n",
       "25%        24.591750      -0.925000      -1.575000      32.375000   \n",
       "50%        31.804000      -0.023000      -0.033000      40.516000   \n",
       "75%        45.017000       0.898000       1.565000      53.390000   \n",
       "max       764.408000       2.497000       3.142000     560.271000   \n",
       "\n",
       "         PRI_lep_eta    PRI_lep_phi        PRI_met    PRI_met_phi  \\\n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000   \n",
       "mean       -0.019507       0.043543      41.717235      -0.010119   \n",
       "std         1.264982       1.816611      32.894693       1.812223   \n",
       "min        -2.505000      -3.142000       0.109000      -3.142000   \n",
       "25%        -1.014000      -1.522000      21.398000      -1.575000   \n",
       "50%        -0.045000       0.086000      34.802000      -0.024000   \n",
       "75%         0.959000       1.618000      51.895000       1.561000   \n",
       "max         2.503000       3.142000    2842.617000       3.142000   \n",
       "\n",
       "       PRI_met_sumet    PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  \\\n",
       "count  250000.000000  250000.000000       250000.000000        250000.000000   \n",
       "mean      209.797178       0.979176         -348.329567          -399.254314   \n",
       "std       126.499506       0.977426          532.962789           489.338286   \n",
       "min        13.678000       0.000000         -999.000000          -999.000000   \n",
       "25%       123.017500       0.000000         -999.000000          -999.000000   \n",
       "50%       179.739000       1.000000           38.960000            -1.872000   \n",
       "75%       263.379250       2.000000           75.349000             0.433000   \n",
       "max      2003.976000       3.000000         1120.573000             4.499000   \n",
       "\n",
       "       PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "count        250000.000000          250000.000000           250000.000000   \n",
       "mean           -399.259788            -692.381204             -709.121609   \n",
       "std             489.333883             479.875496              453.384624   \n",
       "min            -999.000000            -999.000000             -999.000000   \n",
       "25%            -999.000000            -999.000000             -999.000000   \n",
       "50%              -2.093000            -999.000000             -999.000000   \n",
       "75%               0.503000              33.703000               -2.457000   \n",
       "max               3.141000             721.456000                4.500000   \n",
       "\n",
       "       PRI_jet_subleading_phi  PRI_jet_all_pt  \n",
       "count           250000.000000   250000.000000  \n",
       "mean              -709.118631       73.064591  \n",
       "std                453.389017       98.015662  \n",
       "min               -999.000000        0.000000  \n",
       "25%               -999.000000        0.000000  \n",
       "50%               -999.000000       40.512500  \n",
       "75%                 -2.275000      109.933750  \n",
       "max                  3.142000     1633.433000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hb = pd.read_csv(DATA_TRAIN_PATH, sep=',')\n",
    "pd.options.display.max_columns = None\n",
    "hb = hb.drop(['Id'], 1)\n",
    "hb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hb = hb.replace(-999, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri0 = hb[hb.PRI_jet_num==0].copy()\n",
    "pri0 = pri0.drop(pri0_to_drop,1)\n",
    "pri0 = pri0.drop([\"PRI_jet_num\",\"PRI_jet_all_pt\"],1)\n",
    "\n",
    "pri1 = hb[hb.PRI_jet_num == 1].copy()\n",
    "pri1 = pri1.drop(pri1_to_drop,1)\n",
    "pri1 = pri1.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "pri2 = hb[hb.PRI_jet_num == 2].copy()\n",
    "pri2 = pri2.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "pri3 = hb[hb.PRI_jet_num == 3].copy()\n",
    "pri3 = pri3.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "pri = [pri0,pri1,pri2,pri3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataSet(dataset):\n",
    "    pri0 = dataset[dataset.PRI_jet_num==0].copy()\n",
    "    pri0 = pri0.drop(pri0_to_drop,1)\n",
    "    pri0 = pri0.drop([\"PRI_jet_num\",\"PRI_jet_all_pt\"],1)\n",
    "\n",
    "    pri1 = dataset[dataset.PRI_jet_num == 1].copy()\n",
    "    pri1 = pri1.drop(pri1_to_drop,1)\n",
    "    pri1 = pri1.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "    pri2 = dataset[dataset.PRI_jet_num == 2].copy()\n",
    "    pri2 = pri2.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "    pri3 = dataset[dataset.PRI_jet_num == 3].copy()\n",
    "    pri3 = pri3.drop([\"PRI_jet_num\"],1)\n",
    "    \n",
    "    return [pri0,pri1,pri2,pri3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPredictions(dataset):\n",
    "    return dataset.Prediction.apply(lambda x: -1 if x == 'b' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset(dataset):\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addX_0_constant(dataset):\n",
    "    return np.c_[np.ones(dataset.shape[0]), dataset.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri = cleanDataSet(hb)\n",
    "predictions = []\n",
    "\n",
    "for idx, dataset in enumerate(pri):\n",
    "    predictions.append(extractPredictions(dataset))\n",
    "    dataset = dataset.drop(['Prediction'],1)\n",
    "    pri[idx] = normalizeDataset(dataset)\n",
    "pri[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[np.ones(X.shape[0]), hb.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  8, 16,  9, 27, 81],\n",
       "       [ 2,  3,  4,  8, 16,  9, 27, 81]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def polynomial_expansion(x, degree = 2):\n",
    "    \"\"\"\n",
    "    Do a polynomial expansion of matrix x up to degree\n",
    "    The minimum degree is 2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        feature matrix x\n",
    "    degree: int\n",
    "        highest degree of the expansion matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        x expanded up to degree\n",
    "    \"\"\"\n",
    "    def poly(x, degree):\n",
    "        \"\"\"\n",
    "        polynomial basis functions for input data x, for j=2 up to j=degree.\n",
    "        \"\"\"\n",
    "        acc = x*x\n",
    "        phi = acc.copy()\n",
    "        for i in range(2, degree):\n",
    "            acc *= x\n",
    "            phi = np.c_[phi, acc]\n",
    "        return phi\n",
    "\n",
    "    for i in range(0,x.shape[1]):\n",
    "        x = np.c_[x, np.apply_along_axis(poly , 0, x[:,i], degree)]\n",
    "    return x\n",
    "\n",
    "polynomial_expansion(np.array([[2,3],[2,3]]),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3404106662758695\n"
     ]
    }
   ],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent.\n",
    "    Uses MSE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n2, e):\n",
    "        return (e.T @ e) / n2\n",
    "    \n",
    "    def compute_gradient(tx, n, e):\n",
    "        return - tx.T @ e / n\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        e = compute_e(y, tx, w)\n",
    "        gradient = compute_gradient(tx, n, e)\n",
    "        loss = compute_loss(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "\n",
    "    return w, loss\n",
    "'''\n",
    "weights = np.array([])\n",
    "for i in range(100):\n",
    "    initial_w = np.full(tX.shape[1], i/100)\n",
    "    max_iters = 100\n",
    "    gamma = 0.3\n",
    "    w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "    weights = np.append(weights, loss)\n",
    "idx = np.argmin(weights)\n",
    "'''\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.3\n",
    "w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34672967168835184\n"
     ]
    }
   ],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent.\n",
    "    Uses MAE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n, e):\n",
    "        return 1/n * np.sum(np.abs(e))\n",
    "    \n",
    "    def compute_gradient(tx, n, e):\n",
    "        e = y - tx @ w\n",
    "    \n",
    "        return -1/n*tx.T @ np.sign(e)\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        e = compute_e(y, tx, w)\n",
    "        gradient = compute_gradient(tx, n, e)\n",
    "        loss = compute_loss(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "\n",
    "    return w, loss\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.3\n",
    "w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4915113296368542\n"
     ]
    }
   ],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using stochastic gradient descent.\n",
    "    Uses MAE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n2, e):\n",
    "        return (e.T @ e) / n2\n",
    "    \n",
    "    def compute_gradient(tx, n, e):\n",
    "        return - tx.T @ e / n\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w[:, np.newaxis]\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    data_size = len(y)\n",
    "    shuffled_indices = np.random.permutation(np.arange(data_size))\n",
    "    shuffled_y = y[shuffled_indices]\n",
    "    shuffled_tx = tx[shuffled_indices]\n",
    "    shuffled_y = shuffled_y[:,np.newaxis]\n",
    "    for n_iter, by, btx in zip(range(max_iters), shuffled_y, shuffled_tx):\n",
    "        by = by[np.newaxis]\n",
    "        btx = btx[np.newaxis, :]\n",
    "        e = compute_e(by, btx, w)\n",
    "        gradient = compute_gradient(btx, n, e)\n",
    "        loss = compute_loss(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "    return w, compute_loss(n2, compute_e(y, tx, w[:,0]))\n",
    "\n",
    "initial_w = np.full(tX.shape[1], 0.1)\n",
    "max_iters = 100000\n",
    "gamma = 0.7\n",
    "w, loss = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34040945216155494\n"
     ]
    }
   ],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"\n",
    "    Linear regression using normal equations.\n",
    "    Use MSE loss function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"    \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n2, e):\n",
    "        return (e.T @ e) / n2\n",
    "    \n",
    "    w = la.solve(tx.T @ tx, tx.T @ y)\n",
    "    \n",
    "    return w, compute_loss(y.shape[0]*2, compute_e(y, tx, w))\n",
    "\n",
    "w, loss = least_squares(y, tX)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Ridge regression using normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\" \n",
    "    \n",
    "            \n",
    "    def compute_e(y, tx, w):\n",
    "        return y - tx @ w\n",
    "    \n",
    "    def compute_loss(n2, e):\n",
    "        return (e.T @ e) / n2\n",
    "    \n",
    "    \n",
    "    X = tx.T @ tx\n",
    "    w = la.solve(X + lambda_ * (2*y.shape[0]) * np.eye(X.shape[0]), tx.T @ y)\n",
    "    return w, compute_loss(y.shape[0]*2, compute_e(y, tx, w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.000, Training RMSE=0.583\n",
      "lambda=0.000, Training RMSE=0.583\n",
      "lambda=0.000, Training RMSE=0.583\n",
      "lambda=0.000, Training RMSE=0.583\n",
      "lambda=0.000, Training RMSE=0.583\n",
      "lambda=0.001, Training RMSE=0.583\n",
      "lambda=0.001, Training RMSE=0.583\n",
      "lambda=0.003, Training RMSE=0.583\n",
      "lambda=0.007, Training RMSE=0.584\n",
      "lambda=0.016, Training RMSE=0.584\n",
      "lambda=0.037, Training RMSE=0.586\n",
      "lambda=0.085, Training RMSE=0.590\n",
      "lambda=0.193, Training RMSE=0.598\n",
      "lambda=0.439, Training RMSE=0.613\n",
      "lambda=1.000, Training RMSE=0.634\n",
      "0.3404162668067664\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression_demo(tx, y):\n",
    "\n",
    "    \n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    \n",
    "    rmse_tr = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        weights, loss = ridge_regression(y, tx, lambda_)\n",
    "        rmse_tr.append(np.sqrt(loss))\n",
    "\n",
    "        print(\"lambda={l:.3f}, Training RMSE={tr:.3f}\".format(l=lambda_, tr=rmse_tr[ind]))\n",
    "        \n",
    "\n",
    "ridge_regression_demo(tX, y)\n",
    "\n",
    "w, loss = ridge_regression(y, tX, 0.001)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1 + np.exp(-t))\n",
    "    \n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    return np.sum(np.log(1 + np.exp(tx @ w)) - y * (tx @ w))\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T @ (sigmoid(tx@w) - y)\n",
    "\n",
    "\n",
    "def logistic_regression_step(y, tx, w):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    return calculate_loss(y, tx, w), calculate_gradient(y, tx, w)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def logistic_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Logistic regression using gradient descent or SGD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"  \n",
    "    def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descen using logistic regression.\n",
    "        Return the loss and the updated w.\n",
    "        \"\"\"\n",
    "        loss = calculate_loss(y, tx, w)\n",
    "        gradient = calculate_gradient(y,tx,w)\n",
    "        w = w - gamma * gradient\n",
    "        return loss, w\n",
    "    \n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    #gamma = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    y = y[:,np.newaxis]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "     \n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, losses[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 1e-8\n",
    "w, loss = logistic_regression(y, tX, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def reg_logistic_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression using gradient descent or SGD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"    \n",
    "    def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "        \"\"\"return the loss, gradient\"\"\"\n",
    "        loss, gradient = logistic_regression_step(y, tx, w)\n",
    "        loss     += 2 * lambda_ * la.norm(w)**2\n",
    "        gradient += lambda_ * w\n",
    "\n",
    "        return loss, gradient\n",
    "    \n",
    "    def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descent, using the penalized logistic regression.\n",
    "        Return the loss and updated w.\n",
    "        \"\"\"\n",
    "        loss, gradient = penalized_logistic_regression(y, tx, w, lambda_) \n",
    "        w = w - gamma * gradient \n",
    "\n",
    "        return loss, w\n",
    "    \n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 1e-8\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    y = y[:,np.newaxis]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 0.000001\n",
    "w, loss = reg_logistic_regression(y, tX, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "hbt = pd.read_csv(DATA_TEST_PATH, sep=',')\n",
    "\n",
    "#hbt = hbt.drop(['Id', 'Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbt = hbt.drop(['Prediction'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hbt = hbt.set_index(['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pri = []\n",
    "for i in range(4):\n",
    "    test_pri.append(hbt[hbt.PRI_jet_num==i])\n",
    "# TODO finish me. Clean the data with the same methodes as with hb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "hbt = pd.read_csv(DATA_TEST_PATH, sep=',')\n",
    "hbt = hbt.drop(['Id', 'Prediction'], 1)\n",
    "hbt = hbt.replace(-999, np.nan)\n",
    "hbt = (hbt - hbt.mean()) / hbt.std()\n",
    "hbt = hbt.fillna(0)\n",
    "hbt = (hbt - hbt.mean()) / hbt.std()\n",
    "tX_test = np.c_[np.ones(X_test.shape[0]), hbt.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w[1:], tX_test)#[:, [0, 1, 2, 3, 4, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) # Selected desired columns\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_pred))\n",
    "print(len(y_pred[y_pred > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions scores\n",
    "Best score by technique\n",
    "\n",
    "<ul>\n",
    "    <li>MSE, gradient descent : 0.649</li>\n",
    "    <li>MAE, gradient descent : 0.678 </li>\n",
    "    <li>ridge regression      : 0.664</li>\n",
    "</ul>\n",
    "Best score after not being stupid with bias:\n",
    "\n",
    "* MSE, GD: \n",
    "* MAE, GD: 0.639\n",
    "* LSQ: 0.706\n",
    "* R-REG: 0.730\n",
    "\n",
    "Best score after normalizing test set + putting zero where unknown:\n",
    "\n",
    "* LSQ: 0.747\n",
    "* R-REG: 0.745\n",
    "\n",
    "Feature expansion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
