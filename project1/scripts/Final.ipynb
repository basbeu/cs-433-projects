{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from expansion import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri0_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_leading_pt\",\"PRI_jet_leading_eta\",\"PRI_jet_leading_phi\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]\n",
    "pri1_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "hb = pd.read_csv(DATA_TRAIN_PATH, sep=',')\n",
    "pd.options.display.max_columns = None\n",
    "hb = hb.drop(['Id'], 1)\n",
    "\n",
    "def cleanDataSet(dataset):\n",
    "    #dataset = add_DER_mass_indicator(dataset)\n",
    "    dataset_mass_def = dataset[dataset.DER_mass_MMC != -999].copy()\n",
    "    dataset_mass_not_def = dataset[dataset.DER_mass_MMC == -999].copy()\n",
    "    dataset_mass_not_def = dataset_mass_not_def.drop(['DER_mass_MMC'],1)\n",
    "    def splitOnJetNum(dataset, DER_mass_MMC_is_defined):\n",
    "        dataset = dataset.replace(-999, np.nan)\n",
    "        if(DER_mass_MMC_is_defined):\n",
    "            dataset = nonPolyFeatureExpansion(dataset)\n",
    "\n",
    "\n",
    "        pri0 = dataset[dataset.PRI_jet_num==0].copy()\n",
    "        pri0 = pri0.drop(pri0_to_drop,1)\n",
    "        pri0 = pri0.drop([\"PRI_jet_num\",\"PRI_jet_all_pt\"],1)\n",
    "\n",
    "        pri1 = dataset[dataset.PRI_jet_num == 1].copy()\n",
    "        pri1 = pri1.drop(pri1_to_drop,1)\n",
    "        pri1 = pri1.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "        pri2 = dataset[dataset.PRI_jet_num == 2].copy()\n",
    "        pri2 = pri2.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "        pri3 = dataset[dataset.PRI_jet_num == 3].copy()\n",
    "        pri3 = pri3.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "        return [pri0,pri1,pd.concat([pri2,pri3])]\n",
    "    \n",
    "    return splitOnJetNum(dataset_mass_def, True) + splitOnJetNum(dataset_mass_not_def, False)\n",
    "\n",
    "def extractPredictions(dataset):\n",
    "    return dataset.Prediction.apply(lambda x: -1 if x == 'b' else 1)\n",
    "\n",
    "def add_DER_mass_indicator(data):\n",
    "    data['DER_mass_MMC_present'] = (data.DER_mass_MMC == -999).apply(lambda x : 1 if x else 0)\n",
    "    return data\n",
    "\n",
    "def nonPolyFeatureExpansion(data):\n",
    "    data['Mass_let_tau_sum_pt_ratio'] = data.DER_mass_MMC * data.DER_pt_ratio_lep_tau / (data.DER_sum_pt+1e-10)\n",
    "    return data\n",
    "\n",
    "def normalizeDataset(dataset):\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    return dataset\n",
    "\n",
    "def normalizeDataset_numpy(dataset):\n",
    "    dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis=0)\n",
    "    dataset = np.nan_to_num(dataset)\n",
    "    dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis = 0)\n",
    "    return dataset\n",
    "\n",
    "def tildaNumpy(X):\n",
    "    return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "POLYNOMIAL_EXPANSION_DEGREE = 13\n",
    "\n",
    "tx = cleanDataSet(hb)\n",
    "y = []\n",
    "\n",
    "for idx, dataset in enumerate(tx):\n",
    "    y.append(extractPredictions(dataset))\n",
    "    dataset = dataset.drop(['Prediction'],1)\n",
    "    tx[idx] = tildaNumpy(normalizeDataset_numpy(polynomial_expansion( normalizeDataset(dataset).to_numpy(), POLYNOMIAL_EXPANSION_DEGREE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "\n",
    "number_of_algorithms = 6\n",
    "\n",
    "w = []\n",
    "for i in range(number_of_algorithms):\n",
    "    w.append([])\n",
    "\n",
    "initial_w = [np.zeros(txi.shape[1]) for txi in tx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 0\n",
    "for i, (yi, txi, wi) in enumerate(zip(y, tx, initial_w)):\n",
    "    local_w, loss = least_squares_GD(yi, txi, wi, 1000, 1e-3)\n",
    "    w[algorithm].append(local_w)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 1\n",
    "for i, (yi, txi, wi) in enumerate(zip(y, tx, initial_w)):\n",
    "    local_w, loss = least_squares_SGD(yi, txi, wi, 1000, 1e-3)\n",
    "    w[algorithm].append(local_w)\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ada] *",
   "language": "python",
   "name": "conda-env-.conda-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
